{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19cca6af2be78df",
   "metadata": {},
   "source": [
    "# PokerNet Training Starter Notebook\n",
    "\n",
    "Small starter notebook for training your `PokerNet` model.\n",
    "\n",
    "What this notebook includes:\n",
    "- device setup\n",
    "- model instantiation\n",
    "- dummy batch shape sanity check\n",
    "- a tiny synthetic dataset + dataloader\n",
    "- one training loop skeleton (policy + value loss)\n",
    "- hand/game state handling (with hand-state reset example)\n",
    "\n",
    "> Replace the synthetic dataset with your real poker data pipeline when ready.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f650ddef738d846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:40:20.541151056Z",
     "start_time": "2026-02-25T11:40:20.455400637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cu128\n",
      "cuda available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(\n",
    "    \"cuda available:\", torch.cuda.is_available()\n",
    ")  # ROCm also uses torch.cuda namespace\n",
    "if torch.cuda.is_available():\n",
    "    print(\"device count:\", torch.cuda.device_count())\n",
    "    print(\"device 0:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fdccf79b5fe0d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:29:53.284357977Z",
     "start_time": "2026-02-25T11:29:53.188239315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.10.0+cu128\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c3c8871083a4",
   "metadata": {},
   "source": [
    "## 1) Import your model\n",
    "\n",
    "Option A (recommended): put your model code in a file, e.g. `poker_model.py`, then import it.\n",
    "\n",
    "```python\n",
    "from poker_model import PokerNet\n",
    "```\n",
    "\n",
    "Option B: paste your `PokerNet` class (and helper classes) into a cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "342c19ea05a04a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:29:53.345852322Z",
     "start_time": "2026-02-25T11:29:53.287377122Z"
    }
   },
   "outputs": [],
   "source": [
    "from nn.nn import PokerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a558d0549780f6e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:29:53.431018557Z",
     "start_time": "2026-02-25T11:29:53.368492602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PokerNet\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters / dimensions (adjust to your setup) ---\n",
    "BETS_IN_DIM = 128\n",
    "HAND_STATE_DIM = 32\n",
    "GAME_STATE_DIM = 32\n",
    "NUM_ACTIONS = 3  # fold / call-check / raise-intent\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "\n",
    "# Model init example (matches your PokerNet signature)\n",
    "model = PokerNet(\n",
    "    bets_in_dim=BETS_IN_DIM,\n",
    "    hand_state_dim=HAND_STATE_DIM,\n",
    "    game_state_dim=GAME_STATE_DIM,\n",
    "    state_mode=\"simple\",  # or 'branched'\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "policy_criterion = nn.CrossEntropyLoss()  # expects raw logits + class indices\n",
    "value_criterion = nn.MSELoss()  # example regression loss\n",
    "\n",
    "print(model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd3610c1428a07",
   "metadata": {},
   "source": [
    "## 2) Quick shape sanity check (dummy batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7016ce46130eb402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:29:53.505410046Z",
     "start_time": "2026-02-25T11:29:53.451563449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_logits: (8, 3)\n",
      "value: (8, 1)\n",
      "next_hand_state: (8, 32)\n",
      "next_game_state: (8, 32)\n",
      "probs row sums (should be ~1): tensor([1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    B = 8\n",
    "    cards = torch.randn(B, 4, 4, 13, device=DEVICE)\n",
    "    bets = torch.randn(B, BETS_IN_DIM, device=DEVICE)\n",
    "    hand_state = torch.zeros(B, HAND_STATE_DIM, device=DEVICE)\n",
    "    game_state = torch.zeros(B, GAME_STATE_DIM, device=DEVICE)\n",
    "\n",
    "    action_logits, value, next_hand_state, next_game_state = model(\n",
    "        cards, bets, hand_state, game_state\n",
    "    )\n",
    "\n",
    "print(\"action_logits:\", tuple(action_logits.shape))  # [B, 3]\n",
    "print(\"value:\", tuple(value.shape))  # [B, 1]\n",
    "print(\"next_hand_state:\", tuple(next_hand_state.shape))\n",
    "print(\"next_game_state:\", tuple(next_game_state.shape))\n",
    "\n",
    "probs = torch.softmax(action_logits, dim=1)\n",
    "print(\"probs row sums (should be ~1):\", probs.sum(dim=1)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948049045c652d00",
   "metadata": {},
   "source": [
    "## 3) Synthetic dataset (replace later)\n",
    "\n",
    "This dataset returns:\n",
    "- `cards`: `[4,4,13]`\n",
    "- `bets`: `[bets_in_dim]`\n",
    "- `target_action`: class index in `{0,1,2}`\n",
    "- `target_value`: scalar\n",
    "- `new_hand`: boolean flag (if `True`, reset hand state before using sample)\n",
    "\n",
    "It also returns `sample_id`, useful if you later want persistent per-sample state storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c983f2d330ffb118",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:29:53.625638105Z",
     "start_time": "2026-02-25T11:29:53.524346761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 128)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SyntheticPokerDataset(Dataset):\n",
    "    def __init__(self, n_samples=512, bets_in_dim=128):\n",
    "        self.n_samples = n_samples\n",
    "        self.cards = torch.randn(n_samples, 4, 4, 13)\n",
    "        self.bets = torch.randn(n_samples, bets_in_dim)\n",
    "        self.target_action = torch.randint(0, 3, (n_samples,), dtype=torch.long)\n",
    "        self.target_value = torch.randn(n_samples, 1)\n",
    "        # Randomly mark ~10% as new-hand boundaries (demo only)\n",
    "        self.new_hand = torch.rand(n_samples) < 0.10\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"sample_id\": idx,\n",
    "            \"cards\": self.cards[idx],\n",
    "            \"bets\": self.bets[idx],\n",
    "            \"target_action\": self.target_action[idx],\n",
    "            \"target_value\": self.target_value[idx],\n",
    "            \"new_hand\": self.new_hand[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "train_ds = SyntheticPokerDataset(n_samples=4096, bets_in_dim=BETS_IN_DIM)\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=False\n",
    ")  # keep ordered for state demo\n",
    "len(train_ds), len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ba024d38fa5c1",
   "metadata": {},
   "source": [
    "## 4) Minimal training loop skeleton\n",
    "\n",
    "Notes:\n",
    "- `action_logits` are used directly in `CrossEntropyLoss` (no softmax before loss)\n",
    "- `hand_state` is reset when `new_hand == True`\n",
    "- `next_*_state` are detached before feeding back to avoid backprop across batches/episodes\n",
    "\n",
    "This is a *starter pattern*; for real poker training, youâ€™ll likely manage state per table/seat/episode rather than globally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c24f14aeab38c6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:29:53.689447579Z",
     "start_time": "2026-02-25T11:29:53.641514368Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model, loader, optimizer, policy_criterion, value_criterion, device\n",
    "):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_policy = 0.0\n",
    "    total_value = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # Demo global states (batch-aligned); for real environments you'll manage these per sequence/table/player.\n",
    "    hand_state = None\n",
    "    game_state = None\n",
    "\n",
    "    for batch in loader:\n",
    "        cards = batch[\"cards\"].to(device).float()\n",
    "        bets = batch[\"bets\"].to(device).float()\n",
    "        target_action = batch[\"target_action\"].to(device).long()\n",
    "        target_value = batch[\"target_value\"].to(device).float()\n",
    "        new_hand = batch[\"new_hand\"].to(device)\n",
    "\n",
    "        B = cards.shape[0]\n",
    "\n",
    "        if hand_state is None or hand_state.shape[0] != B:\n",
    "            hand_state = torch.zeros(B, HAND_STATE_DIM, device=device)\n",
    "            game_state = torch.zeros(B, GAME_STATE_DIM, device=device)\n",
    "\n",
    "        # Reset hand state where a new hand starts\n",
    "        # Keep game_state (longer-term memory)\n",
    "        if new_hand.any():\n",
    "            hand_state = hand_state.clone()\n",
    "            hand_state[new_hand] = 0.0\n",
    "\n",
    "        action_logits, value, next_hand_state, next_game_state = model(\n",
    "            cards, bets, hand_state, game_state\n",
    "        )\n",
    "\n",
    "        policy_loss = policy_criterion(action_logits, target_action)\n",
    "        value_loss = value_criterion(value, target_value)\n",
    "        loss = policy_loss + 0.5 * value_loss  # tune weight as needed\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = action_logits.argmax(dim=1)\n",
    "            total_correct += (preds == target_action).sum().item()\n",
    "            total_count += B\n",
    "            total_loss += loss.item() * B\n",
    "            total_policy += policy_loss.item() * B\n",
    "            total_value += value_loss.item() * B\n",
    "\n",
    "            # Feed state forward to next batch (demo pattern)\n",
    "            hand_state = next_hand_state.detach()\n",
    "            game_state = next_game_state.detach()\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / max(total_count, 1),\n",
    "        \"policy_loss\": total_policy / max(total_count, 1),\n",
    "        \"value_loss\": total_value / max(total_count, 1),\n",
    "        \"acc\": total_correct / max(total_count, 1),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdb62aa3043a912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:30:19.484621366Z",
     "start_time": "2026-02-25T11:29:53.691383418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=1.5999 | policy=1.0995 | value=1.0008 | acc=0.346\n",
      "Epoch 02 | loss=1.5962 | policy=1.0978 | value=0.9967 | acc=0.348\n",
      "Epoch 03 | loss=1.5758 | policy=1.0921 | value=0.9674 | acc=0.382\n",
      "Epoch 04 | loss=1.5159 | policy=1.0710 | value=0.8897 | acc=0.431\n",
      "Epoch 05 | loss=1.4117 | policy=1.0268 | value=0.7698 | acc=0.476\n",
      "Epoch 06 | loss=1.3098 | policy=0.9687 | value=0.6823 | acc=0.531\n",
      "Epoch 07 | loss=1.2098 | policy=0.8938 | value=0.6320 | acc=0.582\n",
      "Epoch 08 | loss=1.1242 | policy=0.8229 | value=0.6025 | acc=0.629\n",
      "Epoch 09 | loss=1.0318 | policy=0.7519 | value=0.5600 | acc=0.668\n",
      "Epoch 10 | loss=0.9319 | policy=0.6698 | value=0.5242 | acc=0.720\n",
      "Epoch 11 | loss=0.8440 | policy=0.5923 | value=0.5034 | acc=0.758\n",
      "Epoch 12 | loss=0.7878 | policy=0.5426 | value=0.4905 | acc=0.777\n",
      "Epoch 13 | loss=0.7204 | policy=0.4830 | value=0.4747 | acc=0.803\n",
      "Epoch 14 | loss=0.6640 | policy=0.4326 | value=0.4629 | acc=0.827\n",
      "Epoch 15 | loss=0.6424 | policy=0.4147 | value=0.4553 | acc=0.828\n",
      "Epoch 16 | loss=0.5600 | policy=0.3436 | value=0.4328 | acc=0.864\n",
      "Epoch 17 | loss=0.5122 | policy=0.3065 | value=0.4113 | acc=0.879\n",
      "Epoch 18 | loss=0.4870 | policy=0.2834 | value=0.4071 | acc=0.888\n",
      "Epoch 19 | loss=0.4479 | policy=0.2509 | value=0.3939 | acc=0.901\n",
      "Epoch 20 | loss=0.4477 | policy=0.2493 | value=0.3969 | acc=0.905\n",
      "Epoch 21 | loss=0.4216 | policy=0.2253 | value=0.3927 | acc=0.916\n",
      "Epoch 22 | loss=0.4056 | policy=0.2168 | value=0.3777 | acc=0.919\n",
      "Epoch 23 | loss=0.3866 | policy=0.1998 | value=0.3735 | acc=0.924\n",
      "Epoch 24 | loss=0.4028 | policy=0.2137 | value=0.3782 | acc=0.920\n",
      "Epoch 25 | loss=0.3766 | policy=0.1959 | value=0.3614 | acc=0.926\n",
      "Epoch 26 | loss=0.3715 | policy=0.1923 | value=0.3584 | acc=0.929\n",
      "Epoch 27 | loss=0.3223 | policy=0.1497 | value=0.3452 | acc=0.948\n",
      "Epoch 28 | loss=0.2772 | policy=0.1188 | value=0.3169 | acc=0.963\n",
      "Epoch 29 | loss=0.2508 | policy=0.1011 | value=0.2993 | acc=0.966\n",
      "Epoch 30 | loss=0.2311 | policy=0.0899 | value=0.2824 | acc=0.970\n",
      "Epoch 31 | loss=0.2274 | policy=0.0902 | value=0.2746 | acc=0.970\n",
      "Epoch 32 | loss=0.2285 | policy=0.0925 | value=0.2722 | acc=0.970\n",
      "Epoch 33 | loss=0.2207 | policy=0.0889 | value=0.2637 | acc=0.973\n",
      "Epoch 34 | loss=0.2025 | policy=0.0767 | value=0.2516 | acc=0.976\n",
      "Epoch 35 | loss=0.1669 | policy=0.0509 | value=0.2320 | acc=0.985\n",
      "Epoch 36 | loss=0.1458 | policy=0.0405 | value=0.2106 | acc=0.989\n",
      "Epoch 37 | loss=0.1203 | policy=0.0267 | value=0.1873 | acc=0.992\n",
      "Epoch 38 | loss=0.1020 | policy=0.0214 | value=0.1612 | acc=0.994\n",
      "Epoch 39 | loss=0.1500 | policy=0.0644 | value=0.1713 | acc=0.979\n",
      "Epoch 40 | loss=0.2948 | policy=0.1733 | value=0.2430 | acc=0.945\n",
      "Epoch 41 | loss=0.2089 | policy=0.0896 | value=0.2386 | acc=0.973\n",
      "Epoch 42 | loss=0.1314 | policy=0.0304 | value=0.2020 | acc=0.991\n",
      "Epoch 43 | loss=0.0950 | policy=0.0105 | value=0.1689 | acc=1.000\n",
      "Epoch 44 | loss=0.0825 | policy=0.0070 | value=0.1512 | acc=1.000\n",
      "Epoch 45 | loss=0.0756 | policy=0.0061 | value=0.1390 | acc=1.000\n",
      "Epoch 46 | loss=0.0628 | policy=0.0056 | value=0.1145 | acc=1.000\n",
      "Epoch 47 | loss=0.0592 | policy=0.0050 | value=0.1083 | acc=1.000\n",
      "Epoch 48 | loss=0.0670 | policy=0.0049 | value=0.1242 | acc=1.000\n",
      "Epoch 49 | loss=0.0731 | policy=0.0053 | value=0.1356 | acc=1.000\n",
      "Epoch 50 | loss=0.0721 | policy=0.0052 | value=0.1339 | acc=1.000\n",
      "Epoch 51 | loss=0.0712 | policy=0.0052 | value=0.1319 | acc=1.000\n",
      "Epoch 52 | loss=0.0608 | policy=0.0047 | value=0.1121 | acc=1.000\n",
      "Epoch 53 | loss=0.0710 | policy=0.0162 | value=0.1097 | acc=0.996\n",
      "Epoch 54 | loss=0.4935 | policy=0.3576 | value=0.2718 | acc=0.887\n",
      "Epoch 55 | loss=0.3620 | policy=0.2271 | value=0.2697 | acc=0.914\n",
      "Epoch 56 | loss=0.1711 | policy=0.0724 | value=0.1975 | acc=0.976\n",
      "Epoch 57 | loss=0.0946 | policy=0.0252 | value=0.1386 | acc=0.995\n",
      "Epoch 58 | loss=0.0574 | policy=0.0083 | value=0.0981 | acc=0.999\n",
      "Epoch 59 | loss=0.0392 | policy=0.0043 | value=0.0698 | acc=1.000\n",
      "Epoch 60 | loss=0.0279 | policy=0.0026 | value=0.0507 | acc=1.000\n",
      "Epoch 61 | loss=0.0217 | policy=0.0021 | value=0.0392 | acc=1.000\n",
      "Epoch 62 | loss=0.0181 | policy=0.0019 | value=0.0325 | acc=1.000\n",
      "Epoch 63 | loss=0.0165 | policy=0.0017 | value=0.0296 | acc=1.000\n",
      "Epoch 64 | loss=0.0175 | policy=0.0016 | value=0.0318 | acc=1.000\n",
      "Epoch 65 | loss=0.0251 | policy=0.0019 | value=0.0465 | acc=1.000\n",
      "Epoch 66 | loss=0.0511 | policy=0.0031 | value=0.0960 | acc=1.000\n",
      "Epoch 67 | loss=0.1391 | policy=0.0518 | value=0.1748 | acc=0.983\n",
      "Epoch 68 | loss=0.4390 | policy=0.2900 | value=0.2979 | acc=0.899\n",
      "Epoch 69 | loss=0.2573 | policy=0.1316 | value=0.2514 | acc=0.952\n",
      "Epoch 70 | loss=0.1456 | policy=0.0570 | value=0.1772 | acc=0.981\n",
      "Epoch 71 | loss=0.0840 | policy=0.0186 | value=0.1308 | acc=0.997\n",
      "Epoch 72 | loss=0.0513 | policy=0.0056 | value=0.0915 | acc=1.000\n",
      "Epoch 73 | loss=0.0348 | policy=0.0026 | value=0.0644 | acc=1.000\n",
      "Epoch 74 | loss=0.0280 | policy=0.0021 | value=0.0518 | acc=1.000\n",
      "Epoch 75 | loss=0.0260 | policy=0.0020 | value=0.0481 | acc=1.000\n",
      "Epoch 76 | loss=0.0276 | policy=0.0021 | value=0.0511 | acc=1.000\n",
      "Epoch 77 | loss=0.0323 | policy=0.0024 | value=0.0599 | acc=1.000\n",
      "Epoch 78 | loss=0.0404 | policy=0.0028 | value=0.0753 | acc=1.000\n",
      "Epoch 79 | loss=0.0547 | policy=0.0039 | value=0.1014 | acc=1.000\n",
      "Epoch 80 | loss=0.0752 | policy=0.0076 | value=0.1353 | acc=0.999\n",
      "Epoch 81 | loss=0.3707 | policy=0.2496 | value=0.2422 | acc=0.918\n",
      "Epoch 82 | loss=0.3414 | policy=0.1984 | value=0.2859 | acc=0.928\n",
      "Epoch 83 | loss=0.1627 | policy=0.0604 | value=0.2046 | acc=0.980\n",
      "Epoch 84 | loss=0.1033 | policy=0.0195 | value=0.1675 | acc=0.996\n",
      "Epoch 85 | loss=0.0744 | policy=0.0061 | value=0.1366 | acc=0.999\n",
      "Epoch 86 | loss=0.0615 | policy=0.0033 | value=0.1163 | acc=1.000\n",
      "Epoch 87 | loss=0.0561 | policy=0.0031 | value=0.1061 | acc=1.000\n",
      "Epoch 88 | loss=0.0508 | policy=0.0029 | value=0.0959 | acc=1.000\n",
      "Epoch 89 | loss=0.0447 | policy=0.0026 | value=0.0841 | acc=1.000\n",
      "Epoch 90 | loss=0.0398 | policy=0.0024 | value=0.0748 | acc=1.000\n",
      "Epoch 91 | loss=0.0371 | policy=0.0022 | value=0.0697 | acc=1.000\n",
      "Epoch 92 | loss=0.0372 | policy=0.0021 | value=0.0701 | acc=1.000\n",
      "Epoch 93 | loss=0.0416 | policy=0.0022 | value=0.0789 | acc=1.000\n",
      "Epoch 94 | loss=0.0480 | policy=0.0023 | value=0.0913 | acc=1.000\n",
      "Epoch 95 | loss=0.0553 | policy=0.0028 | value=0.1050 | acc=1.000\n",
      "Epoch 96 | loss=0.0652 | policy=0.0036 | value=0.1232 | acc=1.000\n",
      "Epoch 97 | loss=0.1516 | policy=0.0731 | value=0.1571 | acc=0.980\n",
      "Epoch 98 | loss=0.4498 | policy=0.3141 | value=0.2712 | acc=0.895\n",
      "Epoch 99 | loss=0.2039 | policy=0.1029 | value=0.2020 | acc=0.964\n",
      "Epoch 100 | loss=0.1080 | policy=0.0372 | value=0.1416 | acc=0.989\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    metrics = train_one_epoch(\n",
    "        model, train_loader, optimizer, policy_criterion, value_criterion, DEVICE\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"loss={metrics['loss']:.4f} | \"\n",
    "        f\"policy={metrics['policy_loss']:.4f} | \"\n",
    "        f\"value={metrics['value_loss']:.4f} | \"\n",
    "        f\"acc={metrics['acc']:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b267c492b699e06",
   "metadata": {},
   "source": [
    "## 5) Inference example\n",
    "\n",
    "Use softmax *outside* the model to get probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84c0277a0951e694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:30:19.550555524Z",
     "start_time": "2026-02-25T11:30:19.511732931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: tensor([[5.1270e-04, 9.9949e-01, 3.3736e-07]])\n",
      "chosen action index: 1\n",
      "value: -1.2747899293899536\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    cards = torch.randn(1, 4, 4, 13, device=DEVICE)\n",
    "    bets = torch.randn(1, BETS_IN_DIM, device=DEVICE)\n",
    "    hand_state = torch.zeros(1, HAND_STATE_DIM, device=DEVICE)\n",
    "    game_state = torch.zeros(1, GAME_STATE_DIM, device=DEVICE)\n",
    "\n",
    "    action_logits, value, next_hand_state, next_game_state = model(\n",
    "        cards, bets, hand_state, game_state\n",
    "    )\n",
    "    probs = torch.softmax(action_logits, dim=1)\n",
    "    action = probs.argmax(dim=1)\n",
    "\n",
    "print(\"probs:\", probs.cpu())\n",
    "print(\"chosen action index:\", action.item())\n",
    "print(\"value:\", value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e547c657995d3ba",
   "metadata": {},
   "source": [
    "## Next steps for real training\n",
    "\n",
    "Replace the synthetic dataset with your real poker pipeline and decide how to manage state:\n",
    "- **hand_state reset** on new hand\n",
    "- **game_state carryover** across hands (same opponent/session), or reset between sessions\n",
    "- if using shuffled supervised samples, store state by episode/table/player rather than passing batch-to-batch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
